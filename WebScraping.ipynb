{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "\n",
    "driver = webdriver.Chrome('/opt/google/chrome/chromedriver')\n",
    "base_url = \"https://www.imdb.com/\"\n",
    "\n",
    "Names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_prep(text):\n",
    "    return re.sub('\\n','',str(text)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval of First Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns list data containing infromation of all Celeb on given page\n",
    "def find_base_page(url=\"https://www.imdb.com/list/ls002913270/\"):\n",
    "    driver.get(url)\n",
    "    print(\"loading page\")\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    actors = []\n",
    "    for div in soup.findAll('div', attrs={'class':'lister-item mode-detail'}):\n",
    "        for item in div.findAll('div',attrs={'class':'lister-item-content'}):\n",
    "            p = (item.find('h3')).find('a', href=True)\n",
    "            name = p.text\n",
    "            print(\"Loading page for\"+name)\n",
    "            second_page_link = base_url+p[\"href\"]\n",
    "            try:\n",
    "                s = second_page(second_page_link)\n",
    "                last_page_link = base_url+s\n",
    "                #print(last_page_link)\n",
    "                content = last_page(last_page_link)\n",
    "                content['name'] = name\n",
    "            except:\n",
    "                continue\n",
    "            actors.append(content)\n",
    "        #to extract images of the celeb    \n",
    "        for img in div.findAll('img'):\n",
    "            url = img.get('src')\n",
    "            ext = url.split('.')[-1]\n",
    "            path = \"images/\"+name+\".\"+ext\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrival of Second Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns link to the next page where bio of celeb is placed\n",
    "def second_page(url):\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    p = soup.find('span',attrs={'class':'see-more inline nobr-only'})\n",
    "    #print(p)\n",
    "    return p.find('a', href=True)[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary containing data of Celeb\n",
    "def last_page(url): \n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    about = soup.find_all('div', class_='soda odd')[0].find_all('p')[0].text\n",
    "    about = basic_prep(about)\n",
    "    trademarks = []\n",
    "    s = 0\n",
    "    try:\n",
    "        t = soup.find_all('h4', class_='li_group')[3]\n",
    "        text = t.text\n",
    "        if('Trade' in str(text)):\n",
    "            s = int(re.findall('[0-9]+',t.text)[0])\n",
    "        else:\n",
    "            t = soup.find_all('h4', class_='li_group')[2]\n",
    "            text = t.text\n",
    "            if('Trade' in str(text)):\n",
    "                s = int(re.findall('[0-9]+',t.text)[0])\n",
    "    except:\n",
    "        pass\n",
    "    odd = True\n",
    "    i = 1\n",
    "    j = 1\n",
    "    #print(s)\n",
    "    for k in range(s):\n",
    "        if(odd):\n",
    "            odd = False\n",
    "            text = soup.find_all('div', class_='soda odd')[i].text\n",
    "            trademarks.append(basic_prep(text))\n",
    "            i+=1\n",
    "        else:\n",
    "            odd = True\n",
    "            text = soup.find_all('div', class_='soda odd')[i].text\n",
    "            trademarks.append(basic_prep(text))\n",
    "            #trademarks.append(soup.find_all('div', class_='soda even')[j].text)\n",
    "            j+=1\n",
    "    #print(trademarks)\n",
    "    trivias = []\n",
    "\n",
    "    p = 3\n",
    "    for k in range(p):\n",
    "        try:\n",
    "            text1 = soup.find_all('div', class_='soda odd')[k+i].text\n",
    "            text2 = soup.find_all('div', class_='soda even')[j+k].text\n",
    "            #trivias.append(soup.find_all('div', class_='soda odd')[k+i].text)\n",
    "            #trivias.append(soup.find_all('div', class_='soda even')[j+k].text)\n",
    "            trivias.append(basic_prep(text1))\n",
    "            trivias.append(basic_prep(text2))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    d = dict()\n",
    "    d['about'] = about\n",
    "    d['trademark'] = trademarks\n",
    "    d['trivia'] = trivias\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    return find_base_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading page\n",
      "Loading page for  Shah Rukh Khan\n",
      ".....\n",
      "Loading page for  Aamir Khan\n",
      ".....\n",
      "Loading page for  Salman Khan\n",
      ".....\n",
      "Loading page for  Katrina Kaif\n",
      ".....\n",
      "Loading page for  Kareena Kapoor\n",
      ".....\n",
      "Loading page for  Karisma Kapoor\n",
      ".....\n",
      "Loading page for  Hrithik Roshan\n",
      ".....\n",
      "Loading page for  Priyanka Chopra\n",
      ".....\n",
      "Loading page for  Amitabh Bachchan\n",
      ".....\n",
      "Loading page for  Ranbir Kapoor\n",
      ".....\n",
      "Loading page for  Anushka Sharma\n",
      ".....\n",
      "Loading page for  Imran Khan\n",
      ".....\n",
      "Loading page for  Deepika Padukone\n",
      ".....\n",
      "Loading page for  Ali Zafar\n",
      ".....\n",
      "Loading page for  Akshay Kumar\n",
      ".....\n",
      "Loading page for  Aishwarya Rai Bachchan\n",
      ".....\n",
      "Loading page for  Rani Mukherjee\n",
      ".....\n",
      "Loading page for  John Abraham\n",
      ".....\n",
      "Loading page for  Bipasha Basu\n",
      ".....\n"
     ]
    }
   ],
   "source": [
    "n = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting into DataFrame\n",
    "df = pd.DataFrame(n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(1,len(df)+1):\n",
    "    l.append(i)\n",
    "df['id'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['id','name','about','trademark', 'trivia']]\n",
    "df.columns = ['id','Name', 'About', 'TradeMark', 'Trivia']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping data into csv\n",
    "df.to_csv('new_scraped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping data into SQL Database\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('TestDB1.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE Celeb (id INTEGER PRIMARY KEY, Name text, About text, TradeMark text, Trivia text)')\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "#df = pd.read_csv('new_scraped_data.csv')\n",
    "df.to_sql('Celeb', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT * FROM Celeb\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
